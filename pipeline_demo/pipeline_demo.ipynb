{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMaPP Text Classification Pipeline\n",
    "\n",
    "\n",
    "## About\n",
    "\n",
    "This document provides a quick intro to the basic functionality of the supervised text classification pipeline.\n",
    "\n",
    "Goals: \n",
    "- Make training of supervised models for text classification easier for lab Members\n",
    "- Abstracted enough to take away tedious and repetitive tasks\n",
    "- But light enough to be modifiable and useful for specific use-cases\n",
    "\n",
    "What does it provide:\n",
    "- Quickly load data from common SMaPP formats\n",
    "- Easily build a pipeline that selects best algorithm, tuning parameters and feature-set from common choices with reasonable defaults\n",
    "\n",
    "## Installation\n",
    "\n",
    "The package can be installed directly from GitHub using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/fridolinlinder/projects/smapp_text_classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/smappnyu/smapp_text_classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main classes contained in the package are `DataSet` and `TextClassifier`. Let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smapp_text_classifier.data import DataSet\n",
    "from smapp_text_classifier.models import TextClassifier\n",
    "from smapp_text_classifier.plot import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import some additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All logging (the amount of messages the package gives you about what is going on) is implemented using the standard python logging module. If you want less messages set the logging level to `logging.DEBUG` or `logging.ERROR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
    "np.random.seed(989898)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Setup\n",
    "\n",
    "The goal of this exercise is to train a supervised model that learns the function mapping a set of labels to a set of text documents. We start out with our labeled data in `.csv` and `.json` format. Here's what our directory looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>773692075699306496</td>\n",
       "      <td>725302089048453124</td>\n",
       "      <td>RT @CNN: Singer Stevie Nicks is backing Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>786581360672735232</td>\n",
       "      <td>753594430330900481</td>\n",
       "      <td>RT @Italians4Trump: Hillary Supporters Attack ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>775873669725843456</td>\n",
       "      <td>1452015206</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>757926635404300292</td>\n",
       "      <td>550488178</td>\n",
       "      <td>one thing i know for sure is that Leslie Knope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>742758704165093376</td>\n",
       "      <td>2910845500</td>\n",
       "      <td>RT @HillaryClinton: Trump's rhetoric is shamef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label            tweet_id             user_id  \\\n",
       "0   Neutral  773692075699306496  725302089048453124   \n",
       "1  Negative  786581360672735232  753594430330900481   \n",
       "2  Positive  775873669725843456          1452015206   \n",
       "3  Positive  757926635404300292           550488178   \n",
       "4  Positive  742758704165093376          2910845500   \n",
       "\n",
       "                                                text  \n",
       "0  RT @CNN: Singer Stevie Nicks is backing Hillar...  \n",
       "1  RT @Italians4Trump: Hillary Supporters Attack ...  \n",
       "2  RT @HillaryClinton: How pay-to-play works:\\n\\n...  \n",
       "3  one thing i know for sure is that Leslie Knope...  \n",
       "4  RT @HillaryClinton: Trump's rhetoric is shamef...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clinton = pd.read_csv('clinton_2016.csv')\n",
    "df_clinton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': {...},\n",
      " 'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Thu Sep 08 01:19:00 +0000 2016',\n",
      " 'entities': {...},\n",
      " 'extended_entities': {...},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'filter_level': 'low',\n",
      " 'geo': None,\n",
      " 'id': 773692075699306496,\n",
      " 'id_str': '773692075699306496',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'is_quote_status': False,\n",
      " 'lang': 'en',\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'random_number': 0.3559609594276685,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'retweeted_status': {...},\n",
      " 'source': '<a href=\"https://roundteam.co\" rel=\"nofollow\">RoundTeam</a>',\n",
      " 'stance': 'Neutral',\n",
      " 'text': 'RT @CNN: Singer Stevie Nicks is backing Hillary Clinton, predicting '\n",
      "         'a \"landslide\" in November https://t.co/JE4KdZjzci '\n",
      "         'https://t.co/TZkHCD69…',\n",
      " 'timestamp': {...},\n",
      " 'timestamp_ms': '1473297540007',\n",
      " 'truncated': False,\n",
      " 'user': {...}}\n"
     ]
    }
   ],
   "source": [
    "with open('clinton_2016.json') as infile:\n",
    "    pprint(json.loads(next(infile)), depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Standardizing the Data\n",
    "\n",
    "Data can come as json or in tabular form. Only requirement is one column/field containing text and one containing a label. We can specify a tokenizer that is used for bag-of-words features (and to determine word boundaries for bag-of-character features). The tokenizer can be any function that maps a string to a list of tokens (e.g. `'This is a sentence' -> ['this', 'is', 'a', 'sentence']`). Here we use a tokenizer that was specifically developed for tweets. Here you could also add lemmatizatio or other desired transformations of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataSet` class allows the classification pipeline that we will use later to access all relevant information about the dataset. It is a light wrapper around a pandas dataframe that implements a few basic functions. The class can be instantiated with data from different formats: Files (tabular format, json format) or `pandas.DataFrame` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.json',\n",
    "                  name='clinton',\n",
    "                  field_mapping={'label': 'stance', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The init method of `DataSet` does the following:\n",
    "- Transform to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @CNN: Singer Stevie Nicks is backing Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @EricJafMN: @realDonaldTrump Why did you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Sounds to me like Hillary is describing hersel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @peaceisactive: LeBron James: Why I'm Endor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   Neutral  RT @CNN: Singer Stevie Nicks is backing Hillar...\n",
       "1  Positive  RT @EricJafMN: @realDonaldTrump Why did you ca...\n",
       "2  Positive  RT @HillaryClinton: How pay-to-play works:\\n\\n...\n",
       "3  Negative  Sounds to me like Hillary is describing hersel...\n",
       "4  Positive  RT @peaceisactive: LeBron James: Why I'm Endor..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split into training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: [84, 83, 31, 4, 69, 18, 44, 42, 54, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Train rows: {dataset.train_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: [39, 63, 22, 2, 79, 16, 19, 50, 88, 33]\n"
     ]
    }
   ],
   "source": [
    "print(f'Test rows: {dataset.test_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @PoliticusSarah: Comey Letter Backfires As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@HillaryClinton Dear Hillary, I fail 2 see the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @45_Committee: “Why aren’t I 50 points ahea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryforVA: Jimmy Ochan found refuge in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "39   Neutral  RT @PoliticusSarah: Comey Letter Backfires As ...\n",
       "63  Negative  @HillaryClinton Dear Hillary, I fail 2 see the...\n",
       "22  Negative  RT @45_Committee: “Why aren’t I 50 points ahea...\n",
       "2   Positive  RT @HillaryClinton: How pay-to-play works:\\n\\n...\n",
       "79  Positive  RT @HillaryforVA: Jimmy Ochan found refuge in ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84    Negative\n",
       "83    Negative\n",
       "31    Negative\n",
       "4     Positive\n",
       "69    Negative\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_labels('train')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_=df_clinton, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing data that is already split into train/test. Note that the dataframes could also be files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_clinton.iloc[:700]\n",
    "df_test = df_clinton.iloc[701:]\n",
    "dataset = DataSet(train_input=df_train, test_input=df_test, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.csv', \n",
    "                  name='clinton', \n",
    "                  field_mapping={'label': 'label', 'text': 'text'}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a text pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the classification pipeline. The first time it pre-computes the desired features to allow quick and repeated testing without repeatedly re-vectorizing the text. Instead, the document term matrix is computed once and stored in a file (in the `cache_dir` you provide). Then documents can be vectorized (transformed into a matrix that can be used in statistical models) by loading the corresponding rows of this matrix. \n",
    "\n",
    "When the pipeline is first instantiated, the feature matrices are pre-computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-05 10:18:21,555 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 1).joblib\n",
      "2019-08-05 10:18:21,557 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:18:21,557 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:18:21,558 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:18:21,748 - root - DEBUG - fit_transform took 0.19s\n",
      "2019-08-05 10:18:21,749 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 2).joblib\n",
      "2019-08-05 10:18:21,749 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:18:21,750 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:18:21,750 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:18:22,170 - root - DEBUG - fit_transform took 0.42s\n",
      "2019-08-05 10:18:22,171 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 3).joblib\n",
      "2019-08-05 10:18:22,171 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:18:22,172 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:18:22,172 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:18:22,764 - root - DEBUG - fit_transform took 0.59s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(\n",
    "    dataset=dataset, \n",
    "    algorithm='svm', \n",
    "    feature_set='word_ngrams',\n",
    "    ngram_range=(1, 3),\n",
    "    cache_dir='feature_cache',\n",
    "    tokenize=tokenizer.tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we computed three matrices for uni-, bi-, and tri-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton_word_(1, 1).joblib clinton_word_(1, 3).joblib\n",
      "clinton_word_(1, 2).joblib\n"
     ]
    }
   ],
   "source": [
    "!ls feature_cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If precomputed features exist the pipeline can re-use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(\n",
    "    dataset=dataset, \n",
    "    algorithm='svm', \n",
    "    feature_set='word_ngrams',\n",
    "    ngram_range=(1, 3),\n",
    "    cache_dir='feature_cache',\n",
    "    tokenize=tokenizer.tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-compute the features the `recompute_features` argument can be set to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-05 10:19:40,771 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 1).joblib\n",
      "2019-08-05 10:19:40,772 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:19:40,772 - root - DEBUG - Not loading due to recompute request\n",
      "2019-08-05 10:19:40,774 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:19:40,956 - root - DEBUG - fit_transform took 0.18s\n",
      "2019-08-05 10:19:40,957 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 2).joblib\n",
      "2019-08-05 10:19:40,958 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:19:40,958 - root - DEBUG - Not loading due to recompute request\n",
      "2019-08-05 10:19:40,959 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:19:41,365 - root - DEBUG - fit_transform took 0.41s\n",
      "2019-08-05 10:19:41,366 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 3).joblib\n",
      "2019-08-05 10:19:41,367 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:19:41,367 - root - DEBUG - Not loading due to recompute request\n",
      "2019-08-05 10:19:41,368 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:19:41,878 - root - DEBUG - fit_transform took 0.51s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(\n",
    "    dataset=dataset, \n",
    "    algorithm='svm', \n",
    "    feature_set='word_ngrams',\n",
    "    ngram_range=(1, 3),\n",
    "    cache_dir='feature_cache',\n",
    "    tokenize=tokenizer.tokenize,\n",
    "    recompute_features=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use basic word embedding features, all pre-trained gensim models are available and can be accessed by their name (see https://github.com/RaRe-Technologies/gensim-data for available models). when a model is used for the first time, it's downloaded from the gensim server and stored locally in the gensim data directory (usually in the home directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-05 10:21:45,170 - root - DEBUG - Pre-computing feature_cache/clinton_glove-wiki-gigaword-50_mean.pkl\n",
      "2019-08-05 10:21:45,171 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:21:45,172 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:21:45,189 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:21:45,191 - root - DEBUG - Loading embedding model\n",
      "2019-08-05 10:21:45,468 - smart_open.smart_open_lib - DEBUG - {'uri': '/Users/fridolinlinder/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz', 'mode': 'rb', 'kw': {}}\n",
      "2019-08-05 10:22:25,570 - root - DEBUG - _load_embedding_model took 40.38s\n",
      "2019-08-05 10:22:25,861 - root - DEBUG - fit_transform took 40.69s\n",
      "2019-08-05 10:22:25,862 - root - DEBUG - Pre-computing feature_cache/clinton_glove-wiki-gigaword-50_max.pkl\n",
      "2019-08-05 10:22:25,864 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:22:25,864 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:22:25,866 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:22:25,867 - root - DEBUG - Loading embedding model\n",
      "2019-08-05 10:22:25,996 - smart_open.smart_open_lib - DEBUG - {'uri': '/Users/fridolinlinder/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz', 'mode': 'rb', 'kw': {}}\n",
      "2019-08-05 10:23:05,463 - root - DEBUG - _load_embedding_model took 39.60s\n",
      "2019-08-05 10:23:05,743 - root - DEBUG - fit_transform took 39.88s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(\n",
    "    dataset=dataset, \n",
    "    algorithm='svm', \n",
    "    feature_set='embeddings', \n",
    "    embedding_model_name='glove-wiki-gigaword-50',\n",
    "    tokenize=tokenizer.tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline pre-computes two document-feature matrices. One where each word-vector in a document is averaged to obtain a document vector, one where the maximum of each dimenions is used. Later we can cross-validate over these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use character n-gram features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-05 10:23:40,837 - root - DEBUG - Pre-computing feature_cache/clinton_char_wb_(3, 3).joblib\n",
      "2019-08-05 10:23:40,839 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:23:40,840 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:23:40,841 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:23:41,533 - root - DEBUG - fit_transform took 0.69s\n",
      "2019-08-05 10:23:41,535 - root - DEBUG - Pre-computing feature_cache/clinton_char_wb_(3, 4).joblib\n",
      "2019-08-05 10:23:41,539 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:23:41,541 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:23:41,543 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:23:43,006 - root - DEBUG - fit_transform took 1.47s\n",
      "2019-08-05 10:23:43,008 - root - DEBUG - Pre-computing feature_cache/clinton_char_wb_(3, 5).joblib\n",
      "2019-08-05 10:23:43,009 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:23:43,010 - root - DEBUG - Cache not found\n",
      "2019-08-05 10:23:43,011 - root - DEBUG - Transforming from scratch\n",
      "2019-08-05 10:23:44,713 - root - DEBUG - fit_transform took 1.70s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(\n",
    "    dataset=dataset, \n",
    "    algorithm='random_forest', \n",
    "    feature_set='char_ngrams', \n",
    "    ngram_range=(3, 5),\n",
    "    tokenize=tokenizer.tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functionality is the building of the pipeline and reasonable default parameters for randomized cross validation.\n",
    "\n",
    "This pipeline can be tuned using standard scikit-learn functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "    clf.pipeline, \n",
    "    param_distributions=clf.params,\n",
    "    n_iter=10, \n",
    "    cv=3, \n",
    "    n_jobs=4, \n",
    "    scoring='accuracy', \n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:   16.2s finished\n",
      "2019-08-05 10:25:42,513 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:25:42,724 - root - DEBUG - _load_from_cache took 0.21s\n",
      "2019-08-05 10:25:42,727 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-08-05 10:25:42,737 - root - DEBUG - fit_transform took 0.22s\n"
     ]
    }
   ],
   "source": [
    "X = dataset.get_texts('train')\n",
    "y = dataset.get_labels('train')\n",
    "CV = CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6735668789808917"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-05 10:25:55,009 - root - DEBUG - Transforming from cache\n",
      "2019-08-05 10:25:55,222 - root - DEBUG - _load_from_cache took 0.21s\n",
      "2019-08-05 10:25:55,223 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-08-05 10:25:55,230 - root - DEBUG - transform took 0.22s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.643\n"
     ]
    }
   ],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')\n",
    "y_pred = CV.predict(X_valid)\n",
    "score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_gram range: (3, 3)\n",
      "Best n_estimators (random_forest): 25.00\n"
     ]
    }
   ],
   "source": [
    "best_tuning_params = CV.best_estimator_.get_params()\n",
    "print(f'Best n_gram range: {best_tuning_params[\"vectorize__ngram_range\"]}')\n",
    "print(f'Best n_estimators (random_forest): {best_tuning_params[\"clf__n_estimators\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__min_samples_split</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>param_vectorize__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.867104</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.288157</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.0260949</td>\n",
       "      <td>311</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__min_samples_split': 0.02609494696283190...</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.717703</td>\n",
       "      <td>0.641148</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.039381</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.554156</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.611973</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.0810205</td>\n",
       "      <td>191</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__min_samples_split': 0.08102054365737989...</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.712919</td>\n",
       "      <td>0.645933</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.036363</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.174629</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>0.273530</td>\n",
       "      <td>0.009018</td>\n",
       "      <td>0.0704503</td>\n",
       "      <td>260</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__min_samples_split': 0.07045032438032485...</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.688995</td>\n",
       "      <td>0.650718</td>\n",
       "      <td>0.654459</td>\n",
       "      <td>0.026753</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.867482</td>\n",
       "      <td>0.017298</td>\n",
       "      <td>0.252471</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.0595418</td>\n",
       "      <td>170</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__min_samples_split': 0.05954180911108659...</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.693780</td>\n",
       "      <td>0.645933</td>\n",
       "      <td>0.652866</td>\n",
       "      <td>0.030912</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.541500</td>\n",
       "      <td>0.062147</td>\n",
       "      <td>0.609818</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.0668841</td>\n",
       "      <td>432</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>{'clf__min_samples_split': 0.06688412696051756...</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.712919</td>\n",
       "      <td>0.641148</td>\n",
       "      <td>0.660828</td>\n",
       "      <td>0.037147</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.326636</td>\n",
       "      <td>0.056470</td>\n",
       "      <td>0.559391</td>\n",
       "      <td>0.033276</td>\n",
       "      <td>0.0560541</td>\n",
       "      <td>180</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>{'clf__min_samples_split': 0.05605413642936416...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.641148</td>\n",
       "      <td>0.660828</td>\n",
       "      <td>0.033563</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.540363</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>0.238638</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.0612805</td>\n",
       "      <td>81</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__min_samples_split': 0.06128047366270688...</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.688995</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.028235</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.095779</td>\n",
       "      <td>0.015361</td>\n",
       "      <td>0.277868</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.0567088</td>\n",
       "      <td>223</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__min_samples_split': 0.05670880404405077...</td>\n",
       "      <td>0.623810</td>\n",
       "      <td>0.693780</td>\n",
       "      <td>0.645933</td>\n",
       "      <td>0.654459</td>\n",
       "      <td>0.029204</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.337908</td>\n",
       "      <td>0.012966</td>\n",
       "      <td>0.222622</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>0.00857212</td>\n",
       "      <td>25</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__min_samples_split': 0.00857211899481211...</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.717703</td>\n",
       "      <td>0.645933</td>\n",
       "      <td>0.673567</td>\n",
       "      <td>0.031506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.876324</td>\n",
       "      <td>0.018965</td>\n",
       "      <td>0.663911</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.0762935</td>\n",
       "      <td>300</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__min_samples_split': 0.07629350119401898...</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.717703</td>\n",
       "      <td>0.641148</td>\n",
       "      <td>0.667197</td>\n",
       "      <td>0.035677</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.867104      0.009309         0.288157        0.000416   \n",
       "1       1.554156      0.138557         0.611973        0.009950   \n",
       "2       1.174629      0.010136         0.273530        0.009018   \n",
       "3       0.867482      0.017298         0.252471        0.002699   \n",
       "4       2.541500      0.062147         0.609818        0.006650   \n",
       "5       1.326636      0.056470         0.559391        0.033276   \n",
       "6       0.540363      0.010249         0.238638        0.001941   \n",
       "7       1.095779      0.015361         0.277868        0.016096   \n",
       "8       0.337908      0.012966         0.222622        0.009213   \n",
       "9       1.876324      0.018965         0.663911        0.002699   \n",
       "\n",
       "  param_clf__min_samples_split param_clf__n_estimators  \\\n",
       "0                    0.0260949                     311   \n",
       "1                    0.0810205                     191   \n",
       "2                    0.0704503                     260   \n",
       "3                    0.0595418                     170   \n",
       "4                    0.0668841                     432   \n",
       "5                    0.0560541                     180   \n",
       "6                    0.0612805                      81   \n",
       "7                    0.0567088                     223   \n",
       "8                   0.00857212                      25   \n",
       "9                    0.0762935                     300   \n",
       "\n",
       "  param_vectorize__ngram_range  \\\n",
       "0                       (3, 3)   \n",
       "1                       (3, 5)   \n",
       "2                       (3, 3)   \n",
       "3                       (3, 3)   \n",
       "4                       (3, 4)   \n",
       "5                       (3, 4)   \n",
       "6                       (3, 3)   \n",
       "7                       (3, 3)   \n",
       "8                       (3, 3)   \n",
       "9                       (3, 5)   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf__min_samples_split': 0.02609494696283190...           0.628571   \n",
       "1  {'clf__min_samples_split': 0.08102054365737989...           0.628571   \n",
       "2  {'clf__min_samples_split': 0.07045032438032485...           0.623810   \n",
       "3  {'clf__min_samples_split': 0.05954180911108659...           0.619048   \n",
       "4  {'clf__min_samples_split': 0.06688412696051756...           0.628571   \n",
       "5  {'clf__min_samples_split': 0.05605413642936416...           0.633333   \n",
       "6  {'clf__min_samples_split': 0.06128047366270688...           0.623810   \n",
       "7  {'clf__min_samples_split': 0.05670880404405077...           0.623810   \n",
       "8  {'clf__min_samples_split': 0.00857211899481211...           0.657143   \n",
       "9  {'clf__min_samples_split': 0.07629350119401898...           0.642857   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.717703           0.641148         0.662420        0.039381   \n",
       "1           0.712919           0.645933         0.662420        0.036363   \n",
       "2           0.688995           0.650718         0.654459        0.026753   \n",
       "3           0.693780           0.645933         0.652866        0.030912   \n",
       "4           0.712919           0.641148         0.660828        0.037147   \n",
       "5           0.708134           0.641148         0.660828        0.033563   \n",
       "6           0.688995           0.636364         0.649682        0.028235   \n",
       "7           0.693780           0.645933         0.654459        0.029204   \n",
       "8           0.717703           0.645933         0.673567        0.031506   \n",
       "9           0.717703           0.641148         0.667197        0.035677   \n",
       "\n",
       "   rank_test_score  \n",
       "0                3  \n",
       "1                3  \n",
       "2                7  \n",
       "3                9  \n",
       "4                5  \n",
       "5                5  \n",
       "6               10  \n",
       "7                7  \n",
       "8                1  \n",
       "9                2  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(CV.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validating accross multiple Algorithms and Feature sets\n",
    "\n",
    "We can use a simple loop to check the performance of different algorithms. So far the following four are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['random_forest', 'elasticnet', 'svm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature sets are available (note that if you use `embeddings` you need to provide a gensim embedding model as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = ['embeddings', 'char_ngrams', 'word_ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in algorithms:\n",
    "    for feature_set in feature_sets:\n",
    "        print(f'Fitting {algorithm} with {feature_set}')\n",
    "        \n",
    "        clf = TextClassifier(\n",
    "            dataset=dataset, \n",
    "            algorithm=algorithm, \n",
    "            feature_set=feature_set, \n",
    "            max_n_features=10000, \n",
    "            embedding_model_name='glove-twitter-100'\n",
    "        )\n",
    "\n",
    "        CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "            clf.pipeline,\n",
    "            param_distributions=clf.params,\n",
    "            n_iter=10, \n",
    "            cv=3, \n",
    "            n_jobs=8,\n",
    "            scoring='accuracy', \n",
    "            iid=False\n",
    "        )\n",
    "        X = dataset.get_texts('train')\n",
    "        y = dataset.get_labels('train')\n",
    "        CV = CV.fit(X, y)\n",
    "        print(CV.best_score_)\n",
    "        \n",
    "        y_valid = dataset.get_labels('test')\n",
    "        X_valid = dataset.get_texts('test')\n",
    "        y_pred = CV.predict(X_valid)\n",
    "        score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "        print(f'Best score for {algorithm} with {feature_set} on test set: {score}')\n",
    "        \n",
    "        best_t_params = CV.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case SVM with character n-grams performed best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
