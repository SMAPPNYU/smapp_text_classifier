{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMaPP Text Classification Pipeline\n",
    "\n",
    "\n",
    "## About\n",
    "\n",
    "This document provides a quick intro to the basic functionality of the pipeline.\n",
    "\n",
    "Goals: \n",
    "- Make training of supervised models for text classification easier for lab Members\n",
    "- Abstracted enough to take away tedious and repetitive tasks\n",
    "- But light enough to be modifiable and useful for specific use-cases\n",
    "\n",
    "What does it provide:\n",
    "- Quickly load data from common SMaPP formats\n",
    "- Easily build a pipeline that selects best algorithm, tuning parameters and featureset from common choices with reasonable defaults\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "The package can be installed directly off of GitHub using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/smappnyu/smapp_text_classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main classes contained in the package are `DataSet` and `TextClassifier`. Let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_learning_curve'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8b85102903fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmapp_text_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msmapp_text_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmapp_text_classifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_learning_curve'"
     ]
    }
   ],
   "source": [
    "from smapp_text_classifier.data import DataSet\n",
    "from smapp_text_classifier.models import TextClassifier\n",
    "from smapp_text_classifier import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import some additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All logging is implemented using the standard python logging module. If you want less messages set the logging level to `logging.ERROR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
    "np.random.seed(989898)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Setup\n",
    "\n",
    "The goal of this exercise is to train a supervised model that learns the function mapping a set of labels to a set of text documents. We start out with our labeled data in `.csv` and `.json` format. Here's what our directory looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton_2016.csv    clinton_2016.json   \u001b[1m\u001b[36membedding_models\u001b[m\u001b[m    pipeline_demo.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>773692075699306496</td>\n",
       "      <td>725302089048453124</td>\n",
       "      <td>RT @CNN: Singer Stevie Nicks is backing Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>786581360672735232</td>\n",
       "      <td>753594430330900481</td>\n",
       "      <td>RT @Italians4Trump: Hillary Supporters Attack ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>775873669725843456</td>\n",
       "      <td>1452015206</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>757926635404300292</td>\n",
       "      <td>550488178</td>\n",
       "      <td>one thing i know for sure is that Leslie Knope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>742758704165093376</td>\n",
       "      <td>2910845500</td>\n",
       "      <td>RT @HillaryClinton: Trump's rhetoric is shamef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label            tweet_id             user_id  \\\n",
       "0   Neutral  773692075699306496  725302089048453124   \n",
       "1  Negative  786581360672735232  753594430330900481   \n",
       "2  Positive  775873669725843456          1452015206   \n",
       "3  Positive  757926635404300292           550488178   \n",
       "4  Positive  742758704165093376          2910845500   \n",
       "\n",
       "                                                text  \n",
       "0  RT @CNN: Singer Stevie Nicks is backing Hillar...  \n",
       "1  RT @Italians4Trump: Hillary Supporters Attack ...  \n",
       "2  RT @HillaryClinton: How pay-to-play works:\\n\\n...  \n",
       "3  one thing i know for sure is that Leslie Knope...  \n",
       "4  RT @HillaryClinton: Trump's rhetoric is shamef...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clinton = pd.read_csv('clinton_2016.csv')\n",
    "df_clinton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': {...},\n",
      " 'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Thu Sep 08 01:19:00 +0000 2016',\n",
      " 'entities': {...},\n",
      " 'extended_entities': {...},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'filter_level': 'low',\n",
      " 'geo': None,\n",
      " 'id': 773692075699306496,\n",
      " 'id_str': '773692075699306496',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'is_quote_status': False,\n",
      " 'lang': 'en',\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'random_number': 0.3559609594276685,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'retweeted_status': {...},\n",
      " 'source': '<a href=\"https://roundteam.co\" rel=\"nofollow\">RoundTeam</a>',\n",
      " 'stance': 'Neutral',\n",
      " 'text': 'RT @CNN: Singer Stevie Nicks is backing Hillary Clinton, predicting '\n",
      "         'a \"landslide\" in November https://t.co/JE4KdZjzci '\n",
      "         'https://t.co/TZkHCD69â€¦',\n",
      " 'timestamp': {...},\n",
      " 'timestamp_ms': '1473297540007',\n",
      " 'truncated': False,\n",
      " 'user': {...}}\n"
     ]
    }
   ],
   "source": [
    "with open('clinton_2016.json') as infile:\n",
    "    pprint(json.loads(next(infile)), depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Standardizing the Data\n",
    "\n",
    "Data can come as json or in tabular form. Only requirement is one column/field containing text and one containing a label. We can specify a tokenizer that is used for bag-of-words features (and to determine word boundaries for bag-of-character features). The tokenizer can be any function that maps a string to a list of tokens (e.g. `'This is a sentence' -> ['this', 'is', 'a', 'sentence']`). Here we use a tokenizer that was specifically developed for tweets. Here you could also add lemmatizatio or other desired transformations of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataSet` class allows the classification pipeline that we will use later to access all relevant information about the dataset. It is a light wrapper around a pandas dataframe that implements a few basic functions. The class can be instantiated with data from different formats: Files (tabular format, json format) or `pandas.DataFrame` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.json',\n",
    "                  name='clinton',\n",
    "                  field_mapping={'label': 'stance', 'text': 'text'},\n",
    "                  tokenizer=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The init method of `DataSet` does the following:\n",
    "- Transform to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @CNN: Singer Stevie Nicks is backing Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @EricJafMN: @realDonaldTrump Why did you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Sounds to me like Hillary is describing hersel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @peaceisactive: LeBron James: Why I'm Endor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   Neutral  RT @CNN: Singer Stevie Nicks is backing Hillar...\n",
       "1  Positive  RT @EricJafMN: @realDonaldTrump Why did you ca...\n",
       "2  Positive  RT @HillaryClinton: How pay-to-play works:\\n\\n...\n",
       "3  Negative  Sounds to me like Hillary is describing hersel...\n",
       "4  Positive  RT @peaceisactive: LeBron James: Why I'm Endor..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split into training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: [84, 83, 31, 4, 69, 18, 44, 42, 54, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Train rows: {dataset.train_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: [39, 63, 22, 2, 79, 16, 19, 50, 88, 33]\n"
     ]
    }
   ],
   "source": [
    "print(f'Test rows: {dataset.test_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RichardGrenell: The Clinton camp says Hill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RealJamesWoods: Justice in America today.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @mayaharris_: \"I will not let anyone turn b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @Stonewoodforge: Here Are Hillary Clinton's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "26  Negative  RT @RichardGrenell: The Clinton camp says Hill...\n",
       "86  Negative  RT @RealJamesWoods: Justice in America today.....\n",
       "2   Positive  RT @HillaryClinton: How pay-to-play works:\\n\\n...\n",
       "55  Positive  RT @mayaharris_: \"I will not let anyone turn b...\n",
       "75  Negative  RT @Stonewoodforge: Here Are Hillary Clinton's..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48    Negative\n",
       "6     Negative\n",
       "99    Positive\n",
       "82     Neutral\n",
       "76    Negative\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_labels('train')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_=df_clinton, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'},\n",
    "                  tokenizer=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing data that is already split into train/test. Note that the dataframes could also be files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_clinton.iloc[:700]\n",
    "df_test = df_clinton.iloc[701:]\n",
    "dataset = DataSet(train_input=df_train, test_input=df_test, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'},\n",
    "                  tokenizer=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.csv', \n",
    "                  name='clinton', \n",
    "                  field_mapping={'label': 'label', 'text': 'text'},\n",
    "                  tokenizer=tokenizer.tokenize\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a text pipeline\n",
    "If we want to use embeddings we need to download (or train our own) an embedding model. Here we download a large embedding model trained by Facebook on the common crawl web archive (watch out it's 6GB so the download might take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p embedding_models/\n",
    "# !wget -P embedding_models/ https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# !gunzip embedding_models/cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the classification pipeline. The first time it pre-computes the desired features to allow quick and repeated testing without loading a large number of feature matrices into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-01 09:28:47,659 - root - INFO - Using precomputed features for mean-pooled embeddings\n",
      "2019-04-01 09:28:47,660 - root - INFO - Using precomputed features for max-pooled embeddings\n"
     ]
    }
   ],
   "source": [
    "#this will take 90s when run for the first time\n",
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='embeddings',\n",
    "                     embedding_model=('fasttext', 'embedding_models/cc.en.300.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton_fasttext_max_embedded.p  clinton_word_2.npz\n",
      "clinton_fasttext_mean_embedded.p clinton_word_3.npz\n",
      "clinton_word_1.npz\n"
     ]
    }
   ],
   "source": [
    "!ls feature_cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-01 09:51:56,359 - root - INFO - Precomputing char_ngrams (3)..\n",
      "2019-04-01 09:51:56,391 - root - INFO - Precomputing char_ngrams (4)..\n",
      "2019-04-01 09:51:56,421 - root - INFO - Precomputing char_ngrams (5)..\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(dataset=dataset, algorithm='elasticnet', feature_set='char_ngrams',\n",
    "                    recompute_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton_word_1.npz       clinton_word_2.npz       clinton_word_3.npz\n",
      "clinton_word_1_vocab.pkl clinton_word_2_vocab.pkl clinton_word_3_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls feature_cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If precomputed features exist the pipeline re-uses them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, algorithm='elasticnet', feature_set='embeddings', \n",
    "                     max_n_features=10000, recompute_features=False, ngram_range=(1, 3),\n",
    "                     embedding_model=('fasttext', 'embedding_models/cc.en.300.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorize__ngram_range': [(3, 3), (3, 4), (3, 5)],\n",
       " 'clf__alpha': <scipy.stats._distn_infrastructure.rv_frozen at 0x1c18282630>,\n",
       " 'clf__l1_ratio': <scipy.stats._distn_infrastructure.rv_frozen at 0x1c18282b38>}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorize': PrecomputeVectorizer(cache_dir='feature_cache/',\n",
       "            dataset=<smapp_text_classifier.data.DataSet object at 0x1c2809be80>,\n",
       "            embedding_model_name=None, feature_set='char_ngrams',\n",
       "            ngram_range=None, pooling_method=None),\n",
       " 'reduce': Chi2Reducer(max_n_features=20000),\n",
       " 'clf': SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "        early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "        l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "        n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
       "        power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "        validation_fraction=0.1, verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functionality is the building of the pipeline:\n",
    "\n",
    "And reasonable default parameters for randomized cross validation:\n",
    "\n",
    "This pipeline can be tuned using standard scikit-learn functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "    clf.pipeline, \n",
    "    param_distributions=clf.params,\n",
    "    n_iter=20, \n",
    "    cv=5, \n",
    "    n_jobs=8, \n",
    "    scoring='accuracy', \n",
    "    iid=True, \n",
    "    return_train_score=False,\n",
    "    random_state=12333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.get_texts('train')\n",
    "y = dataset.get_labels('train')\n",
    "CV = CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')\n",
    "y_pred = CV.predict(X_valid)\n",
    "score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tuning_params = CV.best_estimator_.get_params()\n",
    "print(f'Best n_gram range: {best_tuning_params[\"vectorize__ngram_range\"]}')\n",
    "print(f'Best l1_ratio (elastic net): {best_tuning_params[\"clf__l1_ratio\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validating accross multiple Algorithms and Feature sets\n",
    "\n",
    "We can use a simple loop to check the performance of different algorithms. So far the following four are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['random_forest', 'elasticnet', 'svm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature sets are available (note that if you use `embeddings` you need to provide a gensim embedding model as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = ['embeddings', 'char_ngrams', 'word_ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for algorithm in algorithms:\n",
    "    for feature_set in feature_sets:\n",
    "        print(f'Fitting {algorithm} with {feature_set}')\n",
    "        \n",
    "        clf = TextClassifier(\n",
    "            dataset=dataset, \n",
    "            algorithm=algorithm, \n",
    "            feature_set=feature_set, \n",
    "            max_n_features=1e4, \n",
    "            embedding_model=('fasttext', 'embedding_models/cc.en.300.bin')\n",
    "        )\n",
    "\n",
    "        CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "            clf.pipeline,\n",
    "            param_distributions=clf.params,\n",
    "            n_iter=10, \n",
    "            cv=3, \n",
    "            n_jobs=8,\n",
    "            scoring='accuracy', \n",
    "            iid=False\n",
    "        )\n",
    "        X = dataset.get_texts('train')\n",
    "        y = dataset.get_labels('train')\n",
    "        CV = CV.fit(X, y)\n",
    "        print(CV.best_score_)\n",
    "        \n",
    "        y_valid = dataset.get_labels('test')\n",
    "        X_valid = dataset.get_texts('test')\n",
    "        y_pred = CV.predict(X_valid)\n",
    "        score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "        print(f'Best score for {algorithm} with {feature_set} on test set: {score}')\n",
    "        \n",
    "        best_t_params = CV.best_estimator_.get_params()\n",
    "        logging.info(f'Best feature_set: '\n",
    "                     f'\\n\\tngram_range: {best_t_params[\"vectorize__ngram_range\"]}'\n",
    "                     f'\\n\\tpooling_method: {best_t_params[\"vectorize__pooling_method\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
