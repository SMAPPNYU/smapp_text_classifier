{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMaPP Text Classification Pipeline\n",
    "\n",
    "\n",
    "## About\n",
    "\n",
    "This document provides a quick intro to the basic functionality of the pipeline.\n",
    "\n",
    "Goals: \n",
    "- Make training of supervised models for text classification easier for lab Members\n",
    "- Abstracted enough to take away tedious and repetitive tasks\n",
    "- But light enough to be modifiable and useful for specific use-cases\n",
    "\n",
    "What does it provide:\n",
    "- Quickly load data from common SMaPP formats\n",
    "- Easily build a pipeline that selects best algorithm, tuning parameters and featureset from common choices with reasonable defaults\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "The package can be installed directly off of GitHub using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/fridolinlinder/projects/smapp_text_classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/smappnyu/smapp_text_classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main classes contained in the package are `DataSet` and `TextClassifier`. Let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smapp_text_classifier.data import DataSet\n",
    "from smapp_text_classifier.models import TextClassifier\n",
    "from smapp_text_classifier.plot import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import some additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All logging is implemented using the standard python logging module. If you want less messages set the logging level to `logging.ERROR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
    "np.random.seed(989898)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Setup\n",
    "\n",
    "The goal of this exercise is to train a supervised model that learns the function mapping a set of labels to a set of text documents. We start out with our labeled data in `.csv` and `.json` format. Here's what our directory looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb      clinton_2016.json   pipeline_demo.ipynb\n",
      "clinton_2016.csv    \u001b[1m\u001b[36mfeature_cache\u001b[m\u001b[m       \u001b[1m\u001b[36mtest_cache\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>773692075699306496</td>\n",
       "      <td>725302089048453124</td>\n",
       "      <td>RT @CNN: Singer Stevie Nicks is backing Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>786581360672735232</td>\n",
       "      <td>753594430330900481</td>\n",
       "      <td>RT @Italians4Trump: Hillary Supporters Attack ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>775873669725843456</td>\n",
       "      <td>1452015206</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>757926635404300292</td>\n",
       "      <td>550488178</td>\n",
       "      <td>one thing i know for sure is that Leslie Knope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>742758704165093376</td>\n",
       "      <td>2910845500</td>\n",
       "      <td>RT @HillaryClinton: Trump's rhetoric is shamef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label            tweet_id             user_id  \\\n",
       "0   Neutral  773692075699306496  725302089048453124   \n",
       "1  Negative  786581360672735232  753594430330900481   \n",
       "2  Positive  775873669725843456          1452015206   \n",
       "3  Positive  757926635404300292           550488178   \n",
       "4  Positive  742758704165093376          2910845500   \n",
       "\n",
       "                                                text  \n",
       "0  RT @CNN: Singer Stevie Nicks is backing Hillar...  \n",
       "1  RT @Italians4Trump: Hillary Supporters Attack ...  \n",
       "2  RT @HillaryClinton: How pay-to-play works:\\n\\n...  \n",
       "3  one thing i know for sure is that Leslie Knope...  \n",
       "4  RT @HillaryClinton: Trump's rhetoric is shamef...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clinton = pd.read_csv('clinton_2016.csv')\n",
    "df_clinton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': {...},\n",
      " 'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Thu Sep 08 01:19:00 +0000 2016',\n",
      " 'entities': {...},\n",
      " 'extended_entities': {...},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'filter_level': 'low',\n",
      " 'geo': None,\n",
      " 'id': 773692075699306496,\n",
      " 'id_str': '773692075699306496',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'is_quote_status': False,\n",
      " 'lang': 'en',\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'random_number': 0.3559609594276685,\n",
      " 'retweet_count': 0,\n",
      " 'retweeted': False,\n",
      " 'retweeted_status': {...},\n",
      " 'source': '<a href=\"https://roundteam.co\" rel=\"nofollow\">RoundTeam</a>',\n",
      " 'stance': 'Neutral',\n",
      " 'text': 'RT @CNN: Singer Stevie Nicks is backing Hillary Clinton, predicting '\n",
      "         'a \"landslide\" in November https://t.co/JE4KdZjzci '\n",
      "         'https://t.co/TZkHCD69…',\n",
      " 'timestamp': {...},\n",
      " 'timestamp_ms': '1473297540007',\n",
      " 'truncated': False,\n",
      " 'user': {...}}\n"
     ]
    }
   ],
   "source": [
    "with open('clinton_2016.json') as infile:\n",
    "    pprint(json.loads(next(infile)), depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Standardizing the Data\n",
    "\n",
    "Data can come as json or in tabular form. Only requirement is one column/field containing text and one containing a label. We can specify a tokenizer that is used for bag-of-words features (and to determine word boundaries for bag-of-character features). The tokenizer can be any function that maps a string to a list of tokens (e.g. `'This is a sentence' -> ['this', 'is', 'a', 'sentence']`). Here we use a tokenizer that was specifically developed for tweets. Here you could also add lemmatizatio or other desired transformations of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataSet` class allows the classification pipeline that we will use later to access all relevant information about the dataset. It is a light wrapper around a pandas dataframe that implements a few basic functions. The class can be instantiated with data from different formats: Files (tabular format, json format) or `pandas.DataFrame` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.json',\n",
    "                  name='clinton',\n",
    "                  field_mapping={'label': 'stance', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The init method of `DataSet` does the following:\n",
    "- Transform to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @CNN: Singer Stevie Nicks is backing Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @EricJafMN: @realDonaldTrump Why did you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Sounds to me like Hillary is describing hersel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @peaceisactive: LeBron James: Why I'm Endor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   Neutral  RT @CNN: Singer Stevie Nicks is backing Hillar...\n",
       "1  Positive  RT @EricJafMN: @realDonaldTrump Why did you ca...\n",
       "2  Positive  RT @HillaryClinton: How pay-to-play works:\\n\\n...\n",
       "3  Negative  Sounds to me like Hillary is describing hersel...\n",
       "4  Positive  RT @peaceisactive: LeBron James: Why I'm Endor..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split into training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: [84, 83, 31, 4, 69, 18, 44, 42, 54, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Train rows: {dataset.train_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: [39, 63, 22, 2, 79, 16, 19, 50, 88, 33]\n"
     ]
    }
   ],
   "source": [
    "print(f'Test rows: {dataset.test_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @PoliticusSarah: Comey Letter Backfires As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@HillaryClinton Dear Hillary, I fail 2 see the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @45_Committee: “Why aren’t I 50 points ahea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryClinton: How pay-to-play works:\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @HillaryforVA: Jimmy Ochan found refuge in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "39   Neutral  RT @PoliticusSarah: Comey Letter Backfires As ...\n",
       "63  Negative  @HillaryClinton Dear Hillary, I fail 2 see the...\n",
       "22  Negative  RT @45_Committee: “Why aren’t I 50 points ahea...\n",
       "2   Positive  RT @HillaryClinton: How pay-to-play works:\\n\\n...\n",
       "79  Positive  RT @HillaryforVA: Jimmy Ochan found refuge in ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84    Negative\n",
       "83    Negative\n",
       "31    Negative\n",
       "4     Positive\n",
       "69    Negative\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_labels('train')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_=df_clinton, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing data that is already split into train/test. Note that the dataframes could also be files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_clinton.iloc[:700]\n",
    "df_test = df_clinton.iloc[701:]\n",
    "dataset = DataSet(train_input=df_train, test_input=df_test, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.csv', \n",
    "                  name='clinton', \n",
    "                  field_mapping={'label': 'label', 'text': 'text'}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a text pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bag of word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the classification pipeline. The first time it pre-computes the desired features to allow quick and repeated testing without repeatedly re-vectorizing the text. Instead, the document term matrix is computed once and cached to file. Then documents can be vectorized by loading the corresponding rows of this matrix. \n",
    "\n",
    "When the pipeline is first instantiated, the feature matrices are pre-computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:01:48,134 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 1).joblib\n",
      "2019-06-10 14:01:48,135 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:01:48,136 - root - DEBUG - Cache not found\n",
      "2019-06-10 14:01:48,136 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:01:48,283 - root - DEBUG - fit_transform took 0.15s\n",
      "2019-06-10 14:01:48,283 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 2).joblib\n",
      "2019-06-10 14:01:48,284 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:01:48,285 - root - DEBUG - Cache not found\n",
      "2019-06-10 14:01:48,286 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:01:48,668 - root - DEBUG - fit_transform took 0.38s\n",
      "2019-06-10 14:01:48,669 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 3).joblib\n",
      "2019-06-10 14:01:48,669 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:01:48,669 - root - DEBUG - Cache not found\n",
      "2019-06-10 14:01:48,670 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:01:49,192 - root - DEBUG - fit_transform took 0.52s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='word_ngrams',\n",
    "                     ngram_range=(1, 3),\n",
    "                     cache_dir='feature_cache',\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we computed three matrices for uni-, bi-, and tri-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton_word_(1, 1).joblib clinton_word_(1, 3).joblib\n",
      "clinton_word_(1, 2).joblib\n"
     ]
    }
   ],
   "source": [
    "!ls feature_cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If precomputed features exist the pipeline can re-use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='word_ngrams',\n",
    "                     ngram_range=(1, 3),\n",
    "                     cache_dir='feature_cache',\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-compute the features the `recompute_features` argument can be set to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:05:27,019 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 1).joblib\n",
      "2019-06-10 14:05:27,020 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:05:27,021 - root - DEBUG - Not loading due to recompute request\n",
      "2019-06-10 14:05:27,021 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:05:27,259 - root - DEBUG - fit_transform took 0.24s\n",
      "2019-06-10 14:05:27,260 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 2).joblib\n",
      "2019-06-10 14:05:27,261 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:05:27,262 - root - DEBUG - Not loading due to recompute request\n",
      "2019-06-10 14:05:27,262 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:05:27,650 - root - DEBUG - fit_transform took 0.39s\n",
      "2019-06-10 14:05:27,651 - root - DEBUG - Pre-computing feature_cache/clinton_word_(1, 3).joblib\n",
      "2019-06-10 14:05:27,652 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:05:27,653 - root - DEBUG - Not loading due to recompute request\n",
      "2019-06-10 14:05:27,653 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:05:28,177 - root - DEBUG - fit_transform took 0.53s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='word_ngrams',\n",
    "                     ngram_range=(1, 3),\n",
    "                     cache_dir='feature_cache',\n",
    "                     tokenize=tokenizer.tokenize,\n",
    "                     recompute_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use basic word embedding features, all pre-trained gensim models are available and can be accessed by their name (see https://github.com/RaRe-Technologies/gensim-data for available models). when a model is used for the first time, it's downloaded from the gensim server and stored locally in the gensim data directory (usually in the home directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:10:43,250 - root - DEBUG - Pre-computing feature_cache/clinton_glove-twitter-100_mean.pkl\n",
      "2019-06-10 14:10:43,251 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:10:43,251 - root - DEBUG - Cache not found\n",
      "2019-06-10 14:10:43,252 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:10:43,252 - root - DEBUG - Loading embedding model\n",
      "2019-06-10 14:10:43,297 - smart_open.smart_open_lib - DEBUG - {'uri': '/Users/fridolinlinder/gensim-data/glove-twitter-100/glove-twitter-100.gz', 'mode': 'rb', 'kw': {}}\n",
      "2019-06-10 14:12:23,961 - root - DEBUG - _load_embedding_model took 100.71s\n",
      "2019-06-10 14:12:23,962 - root - DEBUG - Embedding documents\n",
      "2019-06-10 14:12:24,355 - root - DEBUG - transform took 101.10s\n",
      "2019-06-10 14:12:24,356 - root - DEBUG - Pre-computing feature_cache/clinton_glove-twitter-100_max.pkl\n",
      "2019-06-10 14:12:24,356 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:12:24,357 - root - DEBUG - Cache not found\n",
      "2019-06-10 14:12:24,357 - root - DEBUG - Transforming from scratch\n",
      "2019-06-10 14:12:24,359 - root - DEBUG - Loading embedding model\n",
      "2019-06-10 14:12:24,411 - smart_open.smart_open_lib - DEBUG - {'uri': '/Users/fridolinlinder/gensim-data/glove-twitter-100/glove-twitter-100.gz', 'mode': 'rb', 'kw': {}}\n",
      "2019-06-10 14:14:05,133 - root - DEBUG - _load_embedding_model took 100.77s\n",
      "2019-06-10 14:14:05,133 - root - DEBUG - Embedding documents\n",
      "2019-06-10 14:14:05,494 - root - DEBUG - transform took 101.14s\n"
     ]
    }
   ],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='embeddings', \n",
    "                     embedding_model_name='glove-twitter-100',\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline pre-computes two document-feature matrices. One where each word-vector in a document is averaged to obtain a document vector, one where the maximum of each dimenions is used. Later we can cross-validate over these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use character n-gram features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='elasticnet', \n",
    "                     feature_set='char_ngrams', \n",
    "                     ngram_range=(3, 5),\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functionality is the building of the pipeline and reasonable default parameters for randomized cross validation.\n",
    "\n",
    "This pipeline can be tuned using standard scikit-learn functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "    clf.pipeline, \n",
    "    param_distributions=clf.params,\n",
    "    n_iter=20, \n",
    "    cv=5, \n",
    "    n_jobs=4, \n",
    "    scoring='accuracy', \n",
    "    iid=True, \n",
    "    return_train_score=False,\n",
    "    random_state=12333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:19:41,903 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:19:42,175 - root - DEBUG - _load_from_cache took 0.27s\n",
      "2019-06-10 14:19:42,176 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:19:42,179 - root - DEBUG - fit_transform took 0.28s\n"
     ]
    }
   ],
   "source": [
    "X = dataset.get_texts('train')\n",
    "y = dataset.get_labels('train')\n",
    "CV = CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894904458598726"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:19:42,309 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:19:42,569 - root - DEBUG - _load_from_cache took 0.26s\n",
      "2019-06-10 14:19:42,570 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:19:42,572 - root - DEBUG - transform took 0.26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695\n"
     ]
    }
   ],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')\n",
    "y_pred = CV.predict(X_valid)\n",
    "score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_gram range: (3, 5)\n",
      "Best l1_ratio (elastic net): 0.12\n"
     ]
    }
   ],
   "source": [
    "best_tuning_params = CV.best_estimator_.get_params()\n",
    "print(f'Best n_gram range: {best_tuning_params[\"vectorize__ngram_range\"]}')\n",
    "print(f'Best l1_ratio (elastic net): {best_tuning_params[\"clf__l1_ratio\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__l1_ratio</th>\n",
       "      <th>param_vectorize__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314328</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.285309</td>\n",
       "      <td>0.019937</td>\n",
       "      <td>0.00452893</td>\n",
       "      <td>0.79552</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.004528929106661639, 'clf__l1_...</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.606299</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.604839</td>\n",
       "      <td>0.646497</td>\n",
       "      <td>0.043341</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.304766</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.263431</td>\n",
       "      <td>0.009517</td>\n",
       "      <td>0.00620494</td>\n",
       "      <td>0.662786</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.006204944906447042, 'clf__l1_...</td>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.645669</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.683121</td>\n",
       "      <td>0.050191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123009</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.108532</td>\n",
       "      <td>0.012287</td>\n",
       "      <td>0.00065232</td>\n",
       "      <td>0.414368</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__alpha': 0.0006523198121056162, 'clf__l1...</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.667197</td>\n",
       "      <td>0.035491</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.275257</td>\n",
       "      <td>0.019906</td>\n",
       "      <td>0.203141</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.00451058</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>{'clf__alpha': 0.004510582561863079, 'clf__l1_...</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.606299</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.604839</td>\n",
       "      <td>0.524194</td>\n",
       "      <td>0.616242</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.231471</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.201713</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.00158527</td>\n",
       "      <td>0.197508</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>{'clf__alpha': 0.0015852703375864763, 'clf__l1...</td>\n",
       "      <td>0.716535</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.674603</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>0.670382</td>\n",
       "      <td>0.028072</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.281163</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.257206</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.0065746</td>\n",
       "      <td>0.254428</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.006574596558343654, 'clf__l1_...</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>0.671975</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.258284</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.00848903</td>\n",
       "      <td>0.188967</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.008489030400038417, 'clf__l1_...</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.620968</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.668790</td>\n",
       "      <td>0.047377</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>0.290169</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.00679951</td>\n",
       "      <td>0.662623</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.006799507177839524, 'clf__l1_...</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.052661</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.336295</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>0.302997</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>0.000446601</td>\n",
       "      <td>0.411518</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.00044660147215268455, 'clf__l...</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.629921</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.035832</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.349309</td>\n",
       "      <td>0.011910</td>\n",
       "      <td>0.314805</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.00920426</td>\n",
       "      <td>0.717592</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.00920425881517373, 'clf__l1_r...</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.051951</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.348215</td>\n",
       "      <td>0.005110</td>\n",
       "      <td>0.310158</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.00726746</td>\n",
       "      <td>0.792727</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.007267464341036027, 'clf__l1_...</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.667197</td>\n",
       "      <td>0.053654</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.133398</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.119886</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.00893686</td>\n",
       "      <td>0.436571</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__alpha': 0.008936861674740103, 'clf__l1_...</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.622047</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.659236</td>\n",
       "      <td>0.055530</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.135247</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.122295</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.00686176</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__alpha': 0.0068617598636566135, 'clf__l1...</td>\n",
       "      <td>0.685039</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.684713</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.373126</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.308987</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.00698589</td>\n",
       "      <td>0.89061</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.006985886838841356, 'clf__l1_...</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.657643</td>\n",
       "      <td>0.041576</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.271449</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.250041</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.00417483</td>\n",
       "      <td>0.384681</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>{'clf__alpha': 0.00417483452566502, 'clf__l1_r...</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.665605</td>\n",
       "      <td>0.048439</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.139398</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.119889</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.00967774</td>\n",
       "      <td>0.40963</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__alpha': 0.009677744376228576, 'clf__l1_...</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.645669</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.676752</td>\n",
       "      <td>0.047459</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.350983</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.310825</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.00152999</td>\n",
       "      <td>0.12286</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.0015299938006493586, 'clf__l1...</td>\n",
       "      <td>0.700787</td>\n",
       "      <td>0.677165</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.689490</td>\n",
       "      <td>0.030359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.335351</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.301402</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.00830124</td>\n",
       "      <td>0.265635</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>{'clf__alpha': 0.008301237263449317, 'clf__l1_...</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.033430</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.137163</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.117304</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.00145036</td>\n",
       "      <td>0.843995</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__alpha': 0.0014503554355396275, 'clf__l1...</td>\n",
       "      <td>0.669291</td>\n",
       "      <td>0.606299</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.646497</td>\n",
       "      <td>0.032430</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.129829</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.112117</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.00494664</td>\n",
       "      <td>0.50196</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>{'clf__alpha': 0.004946638816683983, 'clf__l1_...</td>\n",
       "      <td>0.653543</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.659236</td>\n",
       "      <td>0.039682</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.314328      0.004339         0.285309        0.019937   \n",
       "1        0.304766      0.012662         0.263431        0.009517   \n",
       "2        0.123009      0.003110         0.108532        0.012287   \n",
       "3        0.275257      0.019906         0.203141        0.003667   \n",
       "4        0.231471      0.003356         0.201713        0.005346   \n",
       "5        0.281163      0.003764         0.257206        0.003003   \n",
       "6        0.281250      0.004924         0.258284        0.004115   \n",
       "7        0.294463      0.014638         0.290169        0.009690   \n",
       "8        0.336295      0.009006         0.302997        0.007585   \n",
       "9        0.349309      0.011910         0.314805        0.008609   \n",
       "10       0.348215      0.005110         0.310158        0.006622   \n",
       "11       0.133398      0.002365         0.119886        0.003047   \n",
       "12       0.135247      0.002202         0.122295        0.005650   \n",
       "13       0.373126      0.021483         0.308987        0.004963   \n",
       "14       0.271449      0.001932         0.250041        0.003092   \n",
       "15       0.139398      0.003922         0.119889        0.002674   \n",
       "16       0.350983      0.006761         0.310825        0.007065   \n",
       "17       0.335351      0.008687         0.301402        0.005289   \n",
       "18       0.137163      0.003518         0.117304        0.002654   \n",
       "19       0.129829      0.003850         0.112117        0.001743   \n",
       "\n",
       "   param_clf__alpha param_clf__l1_ratio param_vectorize__ngram_range  \\\n",
       "0        0.00452893             0.79552                       (3, 5)   \n",
       "1        0.00620494            0.662786                       (3, 5)   \n",
       "2        0.00065232            0.414368                       (3, 3)   \n",
       "3        0.00451058            0.982801                       (3, 4)   \n",
       "4        0.00158527            0.197508                       (3, 4)   \n",
       "5         0.0065746            0.254428                       (3, 5)   \n",
       "6        0.00848903            0.188967                       (3, 5)   \n",
       "7        0.00679951            0.662623                       (3, 5)   \n",
       "8       0.000446601            0.411518                       (3, 5)   \n",
       "9        0.00920426            0.717592                       (3, 5)   \n",
       "10       0.00726746            0.792727                       (3, 5)   \n",
       "11       0.00893686            0.436571                       (3, 3)   \n",
       "12       0.00686176              0.1782                       (3, 3)   \n",
       "13       0.00698589             0.89061                       (3, 5)   \n",
       "14       0.00417483            0.384681                       (3, 4)   \n",
       "15       0.00967774             0.40963                       (3, 3)   \n",
       "16       0.00152999             0.12286                       (3, 5)   \n",
       "17       0.00830124            0.265635                       (3, 5)   \n",
       "18       0.00145036            0.843995                       (3, 3)   \n",
       "19       0.00494664             0.50196                       (3, 3)   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'clf__alpha': 0.004528929106661639, 'clf__l1_...           0.637795   \n",
       "1   {'clf__alpha': 0.006204944906447042, 'clf__l1_...           0.708661   \n",
       "2   {'clf__alpha': 0.0006523198121056162, 'clf__l1...           0.669291   \n",
       "3   {'clf__alpha': 0.004510582561863079, 'clf__l1_...           0.661417   \n",
       "4   {'clf__alpha': 0.0015852703375864763, 'clf__l1...           0.716535   \n",
       "5   {'clf__alpha': 0.006574596558343654, 'clf__l1_...           0.685039   \n",
       "6   {'clf__alpha': 0.008489030400038417, 'clf__l1_...           0.677165   \n",
       "7   {'clf__alpha': 0.006799507177839524, 'clf__l1_...           0.653543   \n",
       "8   {'clf__alpha': 0.00044660147215268455, 'clf__l...           0.724409   \n",
       "9   {'clf__alpha': 0.00920425881517373, 'clf__l1_r...           0.669291   \n",
       "10  {'clf__alpha': 0.007267464341036027, 'clf__l1_...           0.661417   \n",
       "11  {'clf__alpha': 0.008936861674740103, 'clf__l1_...           0.661417   \n",
       "12  {'clf__alpha': 0.0068617598636566135, 'clf__l1...           0.685039   \n",
       "13  {'clf__alpha': 0.006985886838841356, 'clf__l1_...           0.653543   \n",
       "14  {'clf__alpha': 0.00417483452566502, 'clf__l1_r...           0.677165   \n",
       "15  {'clf__alpha': 0.009677744376228576, 'clf__l1_...           0.677165   \n",
       "16  {'clf__alpha': 0.0015299938006493586, 'clf__l1...           0.700787   \n",
       "17  {'clf__alpha': 0.008301237263449317, 'clf__l1_...           0.692913   \n",
       "18  {'clf__alpha': 0.0014503554355396275, 'clf__l1...           0.669291   \n",
       "19  {'clf__alpha': 0.004946638816683983, 'clf__l1_...           0.653543   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.606299           0.722222           0.661290   \n",
       "1            0.645669           0.769841           0.637097   \n",
       "2            0.637795           0.730159           0.669355   \n",
       "3            0.606299           0.682540           0.604839   \n",
       "4            0.677165           0.674603           0.645161   \n",
       "5            0.661417           0.738095           0.637097   \n",
       "6            0.661417           0.753968           0.620968   \n",
       "7            0.685039           0.769841           0.653226   \n",
       "8            0.629921           0.706349           0.669355   \n",
       "9            0.653543           0.753968           0.701613   \n",
       "10           0.637795           0.746032           0.701613   \n",
       "11           0.622047           0.753968           0.669355   \n",
       "12           0.677165           0.761905           0.685484   \n",
       "13           0.637795           0.722222           0.677419   \n",
       "14           0.661417           0.746032           0.645161   \n",
       "15           0.645669           0.753968           0.693548   \n",
       "16           0.677165           0.738095           0.685484   \n",
       "17           0.669291           0.738095           0.669355   \n",
       "18           0.606299           0.690476           0.653226   \n",
       "19           0.661417           0.722222           0.661290   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.604839         0.646497        0.043341               18  \n",
       "1            0.653226         0.683121        0.050191                3  \n",
       "2            0.629032         0.667197        0.035491               12  \n",
       "3            0.524194         0.616242        0.054893               20  \n",
       "4            0.637097         0.670382        0.028072               10  \n",
       "5            0.637097         0.671975        0.037611                9  \n",
       "6            0.629032         0.668790        0.047377               11  \n",
       "7            0.612903         0.675159        0.052661                6  \n",
       "8            0.645161         0.675159        0.035832                6  \n",
       "9            0.596774         0.675159        0.051951                6  \n",
       "10           0.588710         0.667197        0.053654               12  \n",
       "11           0.588710         0.659236        0.055530               15  \n",
       "12           0.612903         0.684713        0.047174                2  \n",
       "13           0.596774         0.657643        0.041576               17  \n",
       "14           0.596774         0.665605        0.048439               14  \n",
       "15           0.612903         0.676752        0.047459                5  \n",
       "16           0.645161         0.689490        0.030359                1  \n",
       "17           0.637097         0.681529        0.033430                4  \n",
       "18           0.612903         0.646497        0.032430               18  \n",
       "19           0.596774         0.659236        0.039682               15  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(CV.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validating accross multiple Algorithms and Feature sets\n",
    "\n",
    "We can use a simple loop to check the performance of different algorithms. So far the following four are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['random_forest', 'elasticnet', 'svm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature sets are available (note that if you use `embeddings` you need to provide a gensim embedding model as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = ['embeddings', 'char_ngrams', 'word_ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random_forest with embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:21:55,992 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:21:55,995 - root - DEBUG - _load_from_cache took 0.00s\n",
      "2019-06-10 14:21:55,996 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:21:55,998 - root - DEBUG - transform took 0.01s\n",
      "2019-06-10 14:21:56,707 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:21:56,711 - root - DEBUG - _load_from_cache took 0.00s\n",
      "2019-06-10 14:21:56,711 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:21:56,712 - root - DEBUG - transform took 0.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6193969772917142\n",
      "Best score for random_forest with embeddings on test set: 0.595\n",
      "Fitting random_forest with char_ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:03,670 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:03,938 - root - DEBUG - _load_from_cache took 0.27s\n",
      "2019-06-10 14:22:03,939 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:03,942 - root - DEBUG - fit_transform took 0.27s\n",
      "2019-06-10 14:22:04,534 - root - DEBUG - Transforming from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6942735626946153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:04,781 - root - DEBUG - _load_from_cache took 0.25s\n",
      "2019-06-10 14:22:04,782 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:04,785 - root - DEBUG - transform took 0.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for random_forest with char_ngrams on test set: 0.667\n",
      "Fitting random_forest with word_ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:13,527 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:13,569 - root - DEBUG - _load_from_cache took 0.04s\n",
      "2019-06-10 14:22:13,570 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:13,573 - root - DEBUG - fit_transform took 0.05s\n",
      "2019-06-10 14:22:14,267 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:14,310 - root - DEBUG - _load_from_cache took 0.04s\n",
      "2019-06-10 14:22:14,311 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:14,312 - root - DEBUG - transform took 0.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6401534138376244\n",
      "Best score for random_forest with word_ngrams on test set: 0.629\n",
      "Fitting elasticnet with embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:14,717 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:14,721 - root - DEBUG - _load_from_cache took 0.00s\n",
      "2019-06-10 14:22:14,722 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:14,723 - root - DEBUG - transform took 0.01s\n",
      "2019-06-10 14:22:14,754 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:14,758 - root - DEBUG - _load_from_cache took 0.00s\n",
      "2019-06-10 14:22:14,758 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:14,760 - root - DEBUG - transform took 0.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6082782714361662\n",
      "Best score for elasticnet with embeddings on test set: 0.614\n",
      "Fitting elasticnet with char_ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:18,333 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:18,451 - root - DEBUG - _load_from_cache took 0.12s\n",
      "2019-06-10 14:22:18,452 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:18,454 - root - DEBUG - fit_transform took 0.12s\n",
      "2019-06-10 14:22:18,487 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:18,609 - root - DEBUG - _load_from_cache took 0.12s\n",
      "2019-06-10 14:22:18,610 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:18,612 - root - DEBUG - transform took 0.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6910989595200121\n",
      "Best score for elasticnet with char_ngrams on test set: 0.662\n",
      "Fitting elasticnet with word_ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:20,721 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:20,959 - root - DEBUG - _load_from_cache took 0.24s\n",
      "2019-06-10 14:22:20,960 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:20,963 - root - DEBUG - fit_transform took 0.24s\n",
      "2019-06-10 14:22:20,972 - root - DEBUG - Transforming from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6752107541581225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:21,203 - root - DEBUG - _load_from_cache took 0.23s\n",
      "2019-06-10 14:22:21,204 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:21,207 - root - DEBUG - transform took 0.23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for elasticnet with word_ngrams on test set: 0.652\n",
      "Fitting svm with embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:21,606 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:21,610 - root - DEBUG - _load_from_cache took 0.00s\n",
      "2019-06-10 14:22:21,610 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:21,612 - root - DEBUG - transform took 0.01s\n",
      "2019-06-10 14:22:21,661 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:21,665 - root - DEBUG - _load_from_cache took 0.00s\n",
      "2019-06-10 14:22:21,666 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:21,668 - root - DEBUG - transform took 0.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6114528746107694\n",
      "Best score for svm with embeddings on test set: 0.662\n",
      "Fitting svm with char_ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:27,139 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:27,348 - root - DEBUG - _load_from_cache took 0.21s\n",
      "2019-06-10 14:22:27,349 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:27,352 - root - DEBUG - fit_transform took 0.21s\n",
      "2019-06-10 14:22:27,818 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:28,017 - root - DEBUG - _load_from_cache took 0.20s\n",
      "2019-06-10 14:22:28,017 - root - DEBUG - Checking if cache matches index docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7069795701374648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:28,020 - root - DEBUG - transform took 0.20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for svm with char_ngrams on test set: 0.695\n",
      "Fitting svm with word_ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 14:22:30,919 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:31,059 - root - DEBUG - _load_from_cache took 0.14s\n",
      "2019-06-10 14:22:31,060 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:31,062 - root - DEBUG - fit_transform took 0.14s\n",
      "2019-06-10 14:22:31,161 - root - DEBUG - Transforming from cache\n",
      "2019-06-10 14:22:31,301 - root - DEBUG - _load_from_cache took 0.14s\n",
      "2019-06-10 14:22:31,302 - root - DEBUG - Checking if cache matches index docs\n",
      "2019-06-10 14:22:31,303 - root - DEBUG - transform took 0.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.710207336523126\n",
      "Best score for svm with word_ngrams on test set: 0.657\n"
     ]
    }
   ],
   "source": [
    "for algorithm in algorithms:\n",
    "    for feature_set in feature_sets:\n",
    "        print(f'Fitting {algorithm} with {feature_set}')\n",
    "        \n",
    "        clf = TextClassifier(\n",
    "            dataset=dataset, \n",
    "            algorithm=algorithm, \n",
    "            feature_set=feature_set, \n",
    "            max_n_features=10000, \n",
    "            embedding_model_name='glove-twitter-100'\n",
    "        )\n",
    "\n",
    "        CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "            clf.pipeline,\n",
    "            param_distributions=clf.params,\n",
    "            n_iter=10, \n",
    "            cv=3, \n",
    "            n_jobs=8,\n",
    "            scoring='accuracy', \n",
    "            iid=False\n",
    "        )\n",
    "        X = dataset.get_texts('train')\n",
    "        y = dataset.get_labels('train')\n",
    "        CV = CV.fit(X, y)\n",
    "        print(CV.best_score_)\n",
    "        \n",
    "        y_valid = dataset.get_labels('test')\n",
    "        X_valid = dataset.get_texts('test')\n",
    "        y_pred = CV.predict(X_valid)\n",
    "        score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "        print(f'Best score for {algorithm} with {feature_set} on test set: {score}')\n",
    "        \n",
    "        best_t_params = CV.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case SVM with character n-grams performed best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
