{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMaPP Text Classification Pipeline\n",
    "\n",
    "\n",
    "## About\n",
    "\n",
    "This document provides a quick intro to the basic functionality of the pipeline.\n",
    "\n",
    "Goals: \n",
    "- Make training of supervised models for text classification easier for lab Members\n",
    "- Abstracted enough to take away tedious and repetitive tasks\n",
    "- But light enough to be modifiable and useful for specific use-cases\n",
    "\n",
    "What does it provide:\n",
    "- Quickly load data from common SMaPP formats\n",
    "- Easily build a pipeline that selects best algorithm, tuning parameters and featureset from common choices with reasonable defaults\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "The package can be installed directly off of GitHub using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/fridolinlinder/projects/smapp_text_classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/smappnyu/smapp_text_classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main classes contained in the package are `DataSet` and `TextClassifier`. Let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smapp_text_classifier.data import DataSet\n",
    "from smapp_text_classifier.models import TextClassifier\n",
    "from smapp_text_classifier.plot import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import some additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All logging is implemented using the standard python logging module. If you want less messages set the logging level to `logging.ERROR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
    "np.random.seed(989898)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Setup\n",
    "\n",
    "The goal of this exercise is to train a supervised model that learns the function mapping a set of labels to a set of text documents. We start out with our labeled data in `.csv` and `.json` format. Here's what our directory looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinton = pd.read_csv('clinton_2016.csv')\n",
    "df_clinton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinton_2016.json') as infile:\n",
    "    pprint(json.loads(next(infile)), depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Standardizing the Data\n",
    "\n",
    "Data can come as json or in tabular form. Only requirement is one column/field containing text and one containing a label. We can specify a tokenizer that is used for bag-of-words features (and to determine word boundaries for bag-of-character features). The tokenizer can be any function that maps a string to a list of tokens (e.g. `'This is a sentence' -> ['this', 'is', 'a', 'sentence']`). Here we use a tokenizer that was specifically developed for tweets. Here you could also add lemmatizatio or other desired transformations of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataSet` class allows the classification pipeline that we will use later to access all relevant information about the dataset. It is a light wrapper around a pandas dataframe that implements a few basic functions. The class can be instantiated with data from different formats: Files (tabular format, json format) or `pandas.DataFrame` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.json',\n",
    "                  name='clinton',\n",
    "                  field_mapping={'label': 'stance', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The init method of `DataSet` does the following:\n",
    "- Transform to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split into training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train rows: {dataset.train_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test rows: {dataset.test_idxs[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_labels('train')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_=df_clinton, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing data that is already split into train/test. Note that the dataframes could also be files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_clinton.iloc[:700]\n",
    "df_test = df_clinton.iloc[701:]\n",
    "dataset = DataSet(train_input=df_train, test_input=df_test, name='clinton',\n",
    "                  field_mapping={'label': 'label', 'text': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet(input_='clinton_2016.csv', \n",
    "                  name='clinton', \n",
    "                  field_mapping={'label': 'label', 'text': 'text'}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a text pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the classification pipeline. The first time it pre-computes the desired features to allow quick and repeated testing without repeatedly re-vectorizing the text. Instead, the document term matrix is computed once and cached to file. Then documents can be vectorized by loading the corresponding rows of this matrix. \n",
    "\n",
    "When the pipeline is first instantiated, the feature matrices are pre-computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='word_ngrams',\n",
    "                     ngram_range=(1, 3),\n",
    "                     cache_dir='feature_cache',\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we computed three matrices for uni-, bi-, and tri-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls feature_cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If precomputed features exist the pipeline can re-use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='word_ngrams',\n",
    "                     ngram_range=(1, 3),\n",
    "                     cache_dir='feature_cache',\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-compute the features the `recompute_features` argument can be set to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='word_ngrams',\n",
    "                     ngram_range=(1, 3),\n",
    "                     cache_dir='feature_cache',\n",
    "                     tokenize=tokenizer.tokenize,\n",
    "                     recompute_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use basic word embedding features, all pre-trained gensim models are available and can be accessed by their name (see https://github.com/RaRe-Technologies/gensim-data for available models). when a model is used for the first time, it's downloaded from the gensim server and stored locally in the gensim data directory (usually in the home directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='svm', \n",
    "                     feature_set='embeddings', \n",
    "                     embedding_model_name='glove-twitter-100',\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline pre-computes two document-feature matrices. One where each word-vector in a document is averaged to obtain a document vector, one where the maximum of each dimenions is used. Later we can cross-validate over these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use character n-gram features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TextClassifier(dataset=dataset, \n",
    "                     algorithm='elasticnet', \n",
    "                     feature_set='char_ngrams', \n",
    "                     ngram_range=(3, 5),\n",
    "                     tokenize=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main functionality is the building of the pipeline and reasonable default parameters for randomized cross validation.\n",
    "\n",
    "This pipeline can be tuned using standard scikit-learn functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "    clf.pipeline, \n",
    "    param_distributions=clf.params,\n",
    "    n_iter=20, \n",
    "    cv=5, \n",
    "    n_jobs=4, \n",
    "    scoring='accuracy', \n",
    "    iid=True, \n",
    "    return_train_score=False,\n",
    "    random_state=12333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.get_texts('train')\n",
    "y = dataset.get_labels('train')\n",
    "CV = CV.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')\n",
    "y_pred = CV.predict(X_valid)\n",
    "score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = dataset.get_labels('test')\n",
    "X_valid = dataset.get_texts('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tuning_params = CV.best_estimator_.get_params()\n",
    "print(f'Best n_gram range: {best_tuning_params[\"vectorize__ngram_range\"]}')\n",
    "print(f'Best l1_ratio (elastic net): {best_tuning_params[\"clf__l1_ratio\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validating accross multiple Algorithms and Feature sets\n",
    "\n",
    "We can use a simple loop to check the performance of different algorithms. So far the following four are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['random_forest', 'elasticnet', 'svm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature sets are available (note that if you use `embeddings` you need to provide a gensim embedding model as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = ['embeddings', 'char_ngrams', 'word_ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in algorithms:\n",
    "    for feature_set in feature_sets:\n",
    "        print(f'Fitting {algorithm} with {feature_set}')\n",
    "        \n",
    "        clf = TextClassifier(\n",
    "            dataset=dataset, \n",
    "            algorithm=algorithm, \n",
    "            feature_set=feature_set, \n",
    "            max_n_features=10000, \n",
    "            embedding_model_name='glove-twitter-100'\n",
    "        )\n",
    "\n",
    "        CV = sklearn.model_selection.RandomizedSearchCV(\n",
    "            clf.pipeline,\n",
    "            param_distributions=clf.params,\n",
    "            n_iter=10, \n",
    "            cv=3, \n",
    "            n_jobs=8,\n",
    "            scoring='accuracy', \n",
    "            iid=False\n",
    "        )\n",
    "        X = dataset.get_texts('train')\n",
    "        y = dataset.get_labels('train')\n",
    "        CV = CV.fit(X, y)\n",
    "        print(CV.best_score_)\n",
    "        \n",
    "        y_valid = dataset.get_labels('test')\n",
    "        X_valid = dataset.get_texts('test')\n",
    "        y_pred = CV.predict(X_valid)\n",
    "        score = round(sklearn.metrics.accuracy_score(y_true=y_valid, y_pred=y_pred), 3)\n",
    "        print(f'Best score for {algorithm} with {feature_set} on test set: {score}')\n",
    "        \n",
    "        best_t_params = CV.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case SVM with character n-grams performed best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
